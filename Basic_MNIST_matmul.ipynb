{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, we will create a neural network with tensorflow to recognize handwritten digits using mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>one hot encoding is used on the data. Meaning, as there are 10 digits, 0-9, the digit 3 will be displayed as [0, 0, 0, 1, 0,\n",
    "0, 0, 0, 0, 0]\n",
    "    \n",
    "Each image is of the size 28x28 pixels and it will be flattened to a 1_D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images for training, evaluation and testing are 55000, 5000, 10000 images respectively\n"
     ]
    }
   ],
   "source": [
    "n_train = mnist.train.num_examples\n",
    "n_eval = mnist.validation.num_examples\n",
    "n_test = mnist.test.num_examples\n",
    "\n",
    "print('The number of images for training, evaluation and testing are {}, {}, {} images respectively'.format(n_train,n_eval,n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Lets work on developing our NN. We will create 3 hidden layers with 512,256,128 neurons respectively. The input layer will have 784 neurons (28*28 image so 784 pixels in total). The final output layer will have 10 neurons as we have 10 classes (0-9 digits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout variable represents a threshold at which we eliminate some units at random, in this case, we choose a 50% probability of being eliminated at every training step at the last hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 784])  #the first layers placeholder\n",
    "Y = tf.placeholder(\"float\", [None, 10]) #the output layers placeholder\n",
    "keep_prob = tf.placeholder(tf.float32) # a tensor to control the dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model will change the values of weights, we need to set an initial value for the weights of our inout layer, 3 hidden layers and output layer. As they are connected to each other, we will create 4 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Muneeb\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "'w1': tf.Variable(tf.truncated_normal([784, 512],stddev=0.1)),\n",
    "'w2': tf.Variable(tf.truncated_normal([512, 256],stddev=0.1)),\n",
    "'w3': tf.Variable(tf.truncated_normal([256, 128],stddev=0.1)),\n",
    "'out': tf.Variable(tf.truncated_normal([128, 10],stddev=0.1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inittialy, we use a small constant value for the bias to ensure that the tensors activate in the intial stages and therefore contribute to the back propagation. The bias is also defined for 4 layers, all except the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = {\n",
    "'b1': tf.Variable(tf.constant(0.1, shape=[512])),\n",
    "'b2': tf.Variable(tf.constant(0.1, shape=[256])),\n",
    "'b3': tf.Variable(tf.constant(0.1, shape=[128])),\n",
    "'out': tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the functions for all the layers. It basically is matrix multiplication on the previous\n",
    "layer’s outputs and the current layer’s weights, and add the bias to these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-d6448e6ca6af>:4: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])         #Matrix Multiplication + Bias\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])   #Matrix Multiplication + Bias\n",
    "layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])   #Matrix Multiplication + Bias\n",
    "layer_drop = tf.nn.dropout(layer_3, keep_prob)                      #This defines the dropout for the last layer\n",
    "output_layer = tf.matmul(layer_3, weights['out']) + biases['out']   #Matrix Multiplication + Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in building the model is to define the loss function that we want to optimize. There are many options to use from the tensor flow module we will use cross_entropy\n",
    "\n",
    "We also need to choose the optimization function which will be used to minimize the loss function. Gradient descent optimization is a common method for finding the minimum of a function by taking iterative steps along the gradient in a negative direction. For this we choose the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=Y, logits=output_layer))   #this defines our loss function\n",
    "train_step = tf.train.AdamOptimizer(0.0003).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally we have the reached the training and testing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() #to initialize all tf variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic theory behind the training are these 4 processes:<br>\n",
    "1. Propagate values forward through the network<br>\n",
    "2. Compute the loss<br>\n",
    "3. Propagate values backward through the network<br>\n",
    "4. Update the parameters<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variables are as follows:<br>\n",
    "1. Learning Rate = 0.0003<br>\n",
    "2. Batch size = 128<br>\n",
    "3. dropout value = 0.5 -> 50% chance of dropping value at last hidden layer <br>\n",
    "4. Total Iterations = 1000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get total accuracy score at each batch iteration\n",
    "correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \t| Loss = 3.6396756 \t| Accuracy = 0.1171875\n",
      "Iteration 100 \t| Loss = 0.48735958 \t| Accuracy = 0.875\n",
      "Iteration 200 \t| Loss = 0.31270728 \t| Accuracy = 0.8828125\n",
      "Iteration 300 \t| Loss = 0.47652382 \t| Accuracy = 0.8828125\n",
      "Iteration 400 \t| Loss = 0.2207914 \t| Accuracy = 0.9296875\n",
      "Iteration 500 \t| Loss = 0.3489792 \t| Accuracy = 0.9140625\n",
      "Iteration 600 \t| Loss = 0.24891691 \t| Accuracy = 0.921875\n",
      "Iteration 700 \t| Loss = 0.19936237 \t| Accuracy = 0.953125\n",
      "Iteration 800 \t| Loss = 0.30250138 \t| Accuracy = 0.90625\n",
      "Iteration 900 \t| Loss = 0.25985283 \t| Accuracy = 0.90625\n",
      "\n",
      "Accuracy on test set: 0.9153\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(128)\n",
    "        sess.run(train_step, feed_dict={\n",
    "            X: batch_x, Y: batch_y, keep_prob: 0.5\n",
    "            })\n",
    "        # print loss and accuracy (per minibatch)\n",
    "        if i % 100 == 0:\n",
    "            minibatch_loss, minibatch_accuracy = sess.run(\n",
    "            [cross_entropy, accuracy],\n",
    "            feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0}\n",
    "            )\n",
    "            print(\n",
    "            \"Iteration\",\n",
    "            str(i),\n",
    "            \"\\t| Loss =\",\n",
    "            str(minibatch_loss),\n",
    "            \"\\t| Accuracy =\",\n",
    "            str(minibatch_accuracy)\n",
    "            )\n",
    "            \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: mnist.test.images, Y:\n",
    "    mnist.test.labels, keep_prob: 1.0})\n",
    "    print(\"\\nAccuracy on test set:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>The above accuracy is for the model when it was trained per batch. At the last line, we acquired the accuracy of the model using our testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Lets see our model work with real data. We will have to import some other libraries as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.invert(Image.open(\"dig.png\").convert('L')).ravel()\n",
    "#I am using an image with a black  background and white text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the convert function to change the image into one grayscale color image. We store this as a numpy array and invert it using np.invert, because the current matrix represents black as 0 and white as 255, whereas we need the opposite. Finally, we call ravel to flatten the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image: 0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    prediction = sess.run(tf.argmax(output_layer, 1), feed_dict={X: [img]})\n",
    "print (\"Prediction for test image:\", np.squeeze(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will be all. For the dataset, if you want to use a bigger dataset, the result will be much better as NN's are data hungry. For the image to use when predicting, you can use a sample image from the internet or simply open paint and change the canvas size to 28*28 pixels, turn the background black and using white, draw any digit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_tnsr_1",
   "language": "python",
   "name": "nlp_tnsr_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
